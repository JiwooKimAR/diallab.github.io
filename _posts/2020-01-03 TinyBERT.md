---
layout: post
categories:
- seminar
title: "TinyBERT: Distilling BERT for Natural Language Understanding"
date: "2019-01-03"
tags:
- "Knowledge Distillation"
- "BERT"
- "Model Compression"
author: "Jiwoo Kim"
pdf: "null"
ppt: "https://drive.google.com/open?id=1JLHZNUNwZWb4YrGalvvp1rUbZOmL3AQA"
semester: "Spring 2019"
---

# TinyBERT: Distilling BERT for Natural Language Understanding

Jiwoo Kim

## (1) 소개

 ‘BERT’는 ‘Bidirectional Encoder Representations from Transformers’의 약자로 자연어 이해 모델(Natural language understanding model)이다. 2018년에 J. Devlin이 발표한 논문에서 처음 나왔고 지금까지 많은 NLP 문제(ex. GLUE, SQuAD 등)에서 좋은 성과를 보이고 있다. 또한 BERT는 12개의 multi-head attention 레이어와 1개의 feed-forward neural network 레이어를 가지는 transformer 블록을 12개 포함하는 굉장히 큰 모델이다.

이처럼 BERT 같은 pre-trained 언어 모델은 많은 계산과 좋은 메모리를 필요로 하기 때문에 리소스 제약이 있는 기기에서 이런 모델들을 실행시키는 것이 어렵다. 예를 들어 BERT-Large 모델은 3억 4천만(340M) 개의 파라미터를 가지고 있기 때문에 사람들은 이 모델을 모바일 기기나 실시간으로 결과를 얻어야 하는 곳에서 쓰는 것에 어려움을 가진다. 다시 말하자면, 이러한 모델들의 큰 크기가 넓은 분야에서의 사용을 막는다고 할 수 있다.

 이 논문의 목표는 Knowledge Distillation(KD)을 통해 모델의 정확도(accuracy)를 유지시키면서 1. Inference time을 줄이고, 2. 모델 크기를 줄이는 것이다. 이 논문에서는 multi-head attention에도 KD를 적용함으로써 많은 성능 향상을 이끌어 냈다. 또한 fine-tuning 이전에 data augmentation을 진행한 것도 논문의 contribution 중 하나라고 볼 수 있다.

## (2) 관련 내용

1) BERT (“BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)”에서 소개됨)
   
    BERT는 3가지 중요한 특성을 가지고 있다.

   * Pre-training & Fine-tuning

        BERT는 2 phase에 나누어 학습을 진행한다. 먼저, Pre-training 단계에서, BERT는 Unsupervised task (Masked LM, Next Sentence Prediction)을 매우 큰 텍스트 뭉치에서 진행한다. 다음으로, Fine-tuning 단계에서, BERT는 task specific 학습을 task specific dataset에서 진행한다.
   * Transformers

        BERT는 12개의 Transformer encoder 블록을 가진다. “Transformer”는 “Attention is All You Need(2017)” 논문에서 소개되었다. Transformer encoder block는 12개의 multi-head attention과 1개의 feed-forward neural network로 구성되어 있다.

        따라서 BERT는 매우 큰 모델이다. BERT-Base 모델은 12개의 Transformer 블록, 12개의 multi-head self-attention, 768 dim의 feed-forward neural network로 구성되어 있고, 전체 parameter 수는 1억 천만(110M) 개이다. BERT-Large 모델은 24개의 Transformer 블록, 16개의 multi-head self-attention, 1024 dim의 feed-forward neural network로 구성되어 있고, 전체 parameter 수는 3억 4천만(340M) 개이다.

    * Bidirectional

	    BERT는 openAI GPT와 ELMo와 다르게 양방향으로 학습된다.

2) Knowledge Distillation (“Distilling the Knowledge in a Neural Network (2015)”에서 소개됨)

    Knowledge Distillation (KD)는 2가지 중요한 특성을 가지고 있다. 

    * Softer Softmax
    
        Teacher model (주로 더 큰 모델)은 학습을 마친 모델이다. 따라서 teacher 모델의 output은 hard label이 아니라 soft label이다. KD는 이러한 soft label output의 분포를 강조하기 위해서 “temperature”라는 상수 값을 이용해 더 soft하게 만든다.

    * 2 개의 손실함수(Loss Functions)
  
        Student model (주로 더 작은 모델)은 학습을 진행 중인 모델이다. Student model은 2개의 손실함수를 가진다. 먼저, distillation loss가 있는데, 이는 teacher의 softer soft label과 student 자신의 soft output label 간의 cross-entropy loss이다. 다음으로, correct label loss가 있는데 이는 student의 soft label과 correct label 간의 cross-entropy loss이다.

## (3) TinyBERT

<img src="https://github.com/JiwooKimAR/DIALabPostImg/blob/master/20200103_1.jpg?raw=true">

이 표는 논문에 있고, 논문 내용을 잘 요약해준다. 또한 이전에 나왔던 KD를 BERT에 적용해 모델 압축을 진행한 논문들과 다른 점이 무엇인지도 잘 보여준다.

TinyBERT는 KD를 pre-training 단계에서 embedding layer, attention layer, hidden layer에 적용하고, fine-tuning 단계에서 embedding layer, attention layer, hidden layer, prediction layer에 적용한다. 그리고 TinyBERT는 task specific dataset에 fine-tuning을 적용하기 전에 data augmentation을 진행한다.

다음 그림은 이 모델을 간략히 도식화 한 것이다.

<img src="https://github.com/JiwooKimAR/DIALabPostImg/blob/master/20200103_2.jpg?raw=true">

 TinyBERT는 앞서 말한 두 가지 목표를 잘 성취했다.

<img src="https://github.com/JiwooKimAR/DIALabPostImg/blob/master/20200103_3.jpg?raw=true"> 

이 표는 논문에 있고, TinyBERT가 원래 BERT 모델보다 모델 크기(parameter의 수)를 7배 줄이고, 9배 빠르다는 것을 보여준다.

<img src="https://github.com/JiwooKimAR/DIALabPostImg/blob/master/20200103_4.jpg?raw=true">

이 표는 논문에 있고, TinyBERT가 모델 크기를 줄이고 빠르면서, 정확도도 원래 BERT 모델만큼 유지했다는 것을 보여준다.
