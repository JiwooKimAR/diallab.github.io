---
layout: post
title: "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
description: "This is the paper review of XLNet: Generalized Autoregressive Pretraining for Language Understanding."
date: 2019-07-26
categories: ["seminar"]
tags: []
comments: true
author: "Daejin Kim"
pdf:
ppt: "https://drive.google.com/open?id=1pU9YBIDgJ14pb21vy47G-h0rs1eLkJoB"
---

<!-- Post name should be this form: today-title.md
        For example, 2019-08-02-hyperparameter-optimization.md -->

<!-- Fill the contents where --Fill-- exists -->
<!-- If you don't want to fill the --Fill--(not necessary) part, then remove them all.
        For example, pdf: -->
<!-- The example is in '_posts/2019-08-02-hyperparameter-optimization.md'>

<!-- For 'title' front matter, follow this format: This is Title Format -->
<!-- For 'description' front matter, follow this format: It is description. -->
<!-- For 'date' front matter, follow this format: 2019-01-01 -->
<!-- For 'tags' front matter, write down the tag in abbreviation
        For example, write down CV instead of Computer Science
        'tags' can be more than one. Follow the format: ["CV", "ML"] -->
<!-- For 'author' fron matter, write down your name in this format: Gildong Hong -->
<!-- For 'pdf' and 'ppt' front matter, if you have the attachment files, write down the url -->

## Abstract
This is the paper review of XLNet: Generalized Autoregressive Pretraining for Language Understanding (Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le).

## Index
1. About XLNet
2. Attention
3. Transformer
4. Autoregressive(AR) Language Model
5. Autoencoding(AE) Language Model
6. Limitations
7. Permutation Language Model
8. Target-Aware Representation
9. Two-Stream Self-Attention
10. Partial Prediction
11. Incorporating Ideas from Transformer-XL
12. Relative Positional Encoding
13. Segment Recurrence
14. Experiments
15. Ablation Study
16. Related Works

<!-- You can add more information below -->